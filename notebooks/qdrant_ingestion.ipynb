{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c3efb01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import uuid\n",
    "import json\n",
    "import pandas as pd\n",
    "from qdrant_client import QdrantClient, models\n",
    "from fastembed import TextEmbedding\n",
    "from tqdm import tqdm\n",
    "from qdrant_client.models import PointStruct, SparseVector\n",
    "from fastembed import SparseTextEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1ab4d6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "DENSE_EMBEDDING_MODEL_NAME = \"BAAI/bge-small-en-v1.5\"\n",
    "DENSE_EMBEDDING_SIZE = 384\n",
    "\n",
    "SPARSE_EMBEDDING_MODEL_NAME = \"prithivida/Splade_PP_en_v1\"\n",
    "\n",
    "client = QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "def create_or_recreate_collection(name: str, indexes: dict, use_hnsw_optimization: bool = False):\n",
    "    \"\"\"Creates a new Qdrant collection with advanced options.\"\"\"\n",
    "    try:\n",
    "        client.get_collection(collection_name=name)\n",
    "        print(f\"Collection '{name}' already exists. Recreating it for a clean slate.\")\n",
    "        client.delete_collection(collection_name=name)\n",
    "    except Exception:\n",
    "        pass  # Collection does not exist, which is fine\n",
    "\n",
    "    print(f\"Creating collection '{name}'...\")\n",
    "    \n",
    "    hnsw_config = None\n",
    "    if use_hnsw_optimization:\n",
    "        print(f\"Applying HNSW optimization for collection '{name}'.\")\n",
    "        hnsw_config = models.HnswConfigDiff(\n",
    "            payload_m=16,\n",
    "            m=0,\n",
    "            on_disk=True,\n",
    "        )\n",
    "\n",
    "    client.create_collection(\n",
    "        collection_name=name,\n",
    "        vectors_config={\n",
    "            \"dense\": models.VectorParams(\n",
    "                size=DENSE_EMBEDDING_SIZE,\n",
    "                distance=models.Distance.COSINE,\n",
    "                on_disk=True,\n",
    "            ),\n",
    "        },\n",
    "        sparse_vectors_config={\n",
    "            \"sparse\": models.SparseVectorParams(\n",
    "                index=models.SparseIndexParams(\n",
    "                    on_disk=False,\n",
    "                )\n",
    "            )\n",
    "        },\n",
    "        hnsw_config=hnsw_config,\n",
    "        on_disk_payload=True,\n",
    "    )\n",
    "    \n",
    "    print(f\"Creating payload indexes for '{name}'...\")\n",
    "    for field, field_type in indexes.items():\n",
    "        # The field_schema now includes the is_tenant flag\n",
    "        client.create_payload_index(name, field, field_schema=field_type)\n",
    "    print(f\"Collection '{name}' created successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0d1c48c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collection 'user_data' already exists. Recreating it for a clean slate.\n",
      "Creating collection 'user_data'...\n",
      "Applying HNSW optimization for collection 'user_data'.\n",
      "Creating payload indexes for 'user_data'...\n",
      "Collection 'user_data' created successfully.\n",
      "Collection 'knowledge_base' already exists. Recreating it for a clean slate.\n",
      "Creating collection 'knowledge_base'...\n",
      "Applying HNSW optimization for collection 'knowledge_base'.\n",
      "Creating payload indexes for 'knowledge_base'...\n",
      "Collection 'knowledge_base' created successfully.\n"
     ]
    }
   ],
   "source": [
    "user_data_indexes = {\n",
    "    \"tenant_id\": models.KeywordIndexParams(\n",
    "        type='keyword',\n",
    "        is_tenant=True,\n",
    "        on_disk=True\n",
    "    ),\n",
    "    \"customer_id\": models.KeywordIndexParams(\n",
    "        type='keyword',\n",
    "        is_tenant=True,\n",
    "        on_disk=True\n",
    "    )\n",
    "}\n",
    "\n",
    "kb_indexes = {\n",
    "    \"tenant_id\": models.KeywordIndexParams(\n",
    "        type='keyword',\n",
    "        is_tenant=True,\n",
    "        on_disk=True\n",
    "    ),\n",
    "    \"tags\": models.KeywordIndexParams(\n",
    "        type='keyword',\n",
    "        on_disk=True\n",
    "    ),\n",
    "    \"source_type\": models.KeywordIndexParams(\n",
    "        type='keyword',\n",
    "        on_disk=True\n",
    "    )\n",
    "}\n",
    "\n",
    "# Create collections with the new advanced settings\n",
    "create_or_recreate_collection(\"user_data\", indexes=user_data_indexes, use_hnsw_optimization=True)\n",
    "create_or_recreate_collection(\"knowledge_base\", indexes=kb_indexes, use_hnsw_optimization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ecd74637",
   "metadata": {},
   "outputs": [],
   "source": [
    "DENSE_EMBEDDING_MODEL_NAME = \"BAAI/bge-small-en-v1.5\"\n",
    "SPARSE_EMBEDDING_MODEL_NAME = \"prithivida/Splade_PP_en_v1\"\n",
    "\n",
    "DENSE_EMBEDDING_MODEL = TextEmbedding(model_name=DENSE_EMBEDDING_MODEL_NAME)\n",
    "SPARSE_EMBEDDING_MODEL = SparseTextEmbedding(model_name=SPARSE_EMBEDDING_MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7c500c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_unstructured_files(directory_path, tenant_id, points_list):\n",
    "    \"\"\"Dynamically reads all JSON files from a given directory.\"\"\"\n",
    "    for filename in os.listdir(directory_path):\n",
    "        if filename.endswith(\".json\"):\n",
    "            filepath = os.path.join(directory_path, filename)\n",
    "            source_name = filename.split('.')[0]  # e.g., 'faqs', 'policy'\n",
    "            \n",
    "            with open(filepath, 'r') as f:\n",
    "                data = json.load(f)\n",
    "\n",
    "                for item in data:\n",
    "                    category = item.get(\n",
    "                        \"question\", item.get(\"title\", {})\n",
    "                    )\n",
    "                    content = item.get(\n",
    "                        \"answer\", item.get(\n",
    "                            \"content\", item.get(\"description\", \"\")\n",
    "                        )\n",
    "                    )\n",
    "                    tags = item.get(\"tags\", {})\n",
    "                    \n",
    "                    if not content:\n",
    "                        continue\n",
    "\n",
    "                    text_to_embed = content\n",
    "                    # THIS CAN ONLY BE QUESTION/TITLE/POLICY_TYPE\n",
    "                    if source_name == 'faqs':\n",
    "                        text_to_embed = f\"Question: {category}\\nAnswer: {content}\"\n",
    "                    elif source_name == 'handbook':\n",
    "                        text_to_embed = f\"Title: {category}\\nContent: {content}\"\n",
    "                    elif source_name == 'policy':\n",
    "                        text_to_embed = f\"Policy Type: {category}\\nPolicy Description: {content}\"\n",
    "\n",
    "                    payload = {\n",
    "                        \"tenant_id\": tenant_id,\n",
    "                        \"source_type\": source_name,\n",
    "                        \"tags\": tags,\n",
    "                        \"content\": text_to_embed,\n",
    "                    }\n",
    "                    points_list.append((text_to_embed, payload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a38a773a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upsert_in_batch(text, payloads, collection_name, batch_size):\n",
    "    dense = list(DENSE_EMBEDDING_MODEL.embed(text))\n",
    "    sparse = list(SPARSE_EMBEDDING_MODEL.embed(text))\n",
    "\n",
    "    total = len(text)\n",
    "    for i in tqdm(range(0, total, batch_size)):\n",
    "        batch_dense_embeddings = dense[i:i + batch_size]\n",
    "        batch_sparse_embeddings = sparse[i:i + batch_size]\n",
    "        batch_payloads = payloads[i:i + batch_size]\n",
    "\n",
    "        points = [\n",
    "            PointStruct(\n",
    "                id=str(uuid.uuid4()),\n",
    "                vector={\n",
    "                    \"dense\": dense_embeddings,\n",
    "                    \"sparse\": SparseVector(indices=sparse_embeddings.indices, values=sparse_embeddings.values)\n",
    "                },\n",
    "                payload=payload\n",
    "            )\n",
    "            for dense_embeddings, sparse_embeddings, payload in zip(batch_dense_embeddings, batch_sparse_embeddings, batch_payloads)\n",
    "        ]\n",
    "\n",
    "        client.upsert(\n",
    "            collection_name=collection_name,\n",
    "            points=points,\n",
    "            wait=True\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e10d37ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ingest_data(data_path, batch_size = 64):\n",
    "    \"\"\"Processes all data sources and uploads them to their respective collections.\"\"\"\n",
    "    user_data_points = []\n",
    "    kb_points = []\n",
    "\n",
    "    tenants = [\"ecom\", \"fintech\"]\n",
    "    for tenant in tenants:\n",
    "        print(f\"\\n--- Processing data for tenant: {tenant} ---\")\n",
    "        \n",
    "        crm_df = pd.read_csv(f\"{data_path}/{tenant}/crm_records.csv\")\n",
    "        for _, row in crm_df.iterrows():\n",
    "            text_to_embed = f\"Customer: {row['name']}, Email: {row['email']}\"\n",
    "            payload = {\"tenant_id\": tenant, \"source_type\": \"crm\", **row.to_dict(), \"text_embeded\": text_to_embed}\n",
    "            user_data_points.append((text_to_embed, payload))\n",
    "\n",
    "        helpdesk_df = pd.read_csv(f\"{data_path}/{tenant}/helpdesk_logs.csv\")\n",
    "        for _, row in helpdesk_df.iterrows():\n",
    "            text_to_embed = f\"Ticket: {row['issue_summary']}, Status: {row['status']}\"\n",
    "            payload = {\"tenant_id\": tenant, \"source_type\": \"helpdesk\", **row.to_dict(), \"text_embeded\": text_to_embed}\n",
    "            user_data_points.append((text_to_embed, payload))\n",
    "\n",
    "        kb_path = f\"{data_path}/{tenant}/knowledge_base\"\n",
    "        process_unstructured_files(kb_path, tenant, kb_points)\n",
    "\n",
    "    # --- Upload to Qdrant ---\n",
    "    # User Data\n",
    "    user_data_texts, user_data_payloads = zip(*user_data_points)\n",
    "    # user_data_embeddings = list(DENSE_EMBEDDING_MODEL.embed(user_data_texts))[0]\n",
    "    upsert_in_batch(user_data_texts, user_data_payloads, \"user_data\", batch_size)\n",
    "    print(f\"\\nIngested {len(user_data_points)} points into 'user_data' collection.\")\n",
    "\n",
    "    # Knowledge Base\n",
    "    kb_texts, kb_payloads = zip(*kb_points)\n",
    "    # kb_embeddings = list(embedding_model.embed(kb_texts))\n",
    "    upsert_in_batch(kb_texts, kb_payloads, \"knowledge_base\", batch_size)\n",
    "    print(f\"Ingested {len(kb_points)} points into 'knowledge_base' collection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "30dc7703",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing data for tenant: ecom ---\n",
      "\n",
      "--- Processing data for tenant: fintech ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [00:00<00:00, 15.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Ingested 300 points into 'user_data' collection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:00<00:00, 15.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ingested 225 points into 'knowledge_base' collection.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ingest_data(data_path = '../data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7aa1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    user_data_indexes = {\n",
    "        \"tenant_id\": models.KeywordIndexParams(\n",
    "            type='keyword',\n",
    "            is_tenant=True,\n",
    "            on_disk=True\n",
    "        ),\n",
    "        \"customer_id\": models.KeywordIndexParams(\n",
    "            type='keyword',\n",
    "            is_tenant=True,\n",
    "            on_disk=True\n",
    "        )\n",
    "    }\n",
    "\n",
    "    kb_indexes = {\n",
    "        \"tenant_id\": models.KeywordIndexParams(\n",
    "            type='keyword',\n",
    "            is_tenant=True,\n",
    "            on_disk=True\n",
    "        ),\n",
    "        \"tags\": models.KeywordIndexParams(\n",
    "            type='keyword',\n",
    "            on_disk=True\n",
    "        ),\n",
    "        \"source_type\": models.KeywordIndexParams(\n",
    "            type='keyword',\n",
    "            on_disk=True\n",
    "        )\n",
    "    }\n",
    "\n",
    "    # Create collections with the new advanced settings\n",
    "    create_or_recreate_collection(\"user_data\", indexes=user_data_indexes, use_hnsw_optimization=True)\n",
    "    create_or_recreate_collection(\"knowledge_base\", indexes=kb_indexes, use_hnsw_optimization=True)\n",
    "    \n",
    "    ingest_data(data_path = '../data')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lyzr-qdrant-multi-agent-system (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
